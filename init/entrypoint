#!/bin/bash

# include bpkg  dependencies
source /usr/local/bin/retry
source /usr/local/bin/bgo
source /usr/local/bin/bgowait

# global variables
GLOBAL_VAR="xyz"


##############################################################################
# validate if all container variables are set
##############################################################################
function validate(){
    vars="SPARK_MASTER_PORT SPARK_MASTER_UI_PORT JMS_IP JMS_PORT APP_NAME PUBLIC_IP METRIC_PATTERN METRIC_REPORTING_INTERVAL"
    for var in $vars; do
        if [[ $(env | awk -F "=" '{print $1}' | grep "^$var$") != "$var" ]]; then
            echo "$var not set but required."
            return 1
        fi
    done
    if [[ -z ${GLOBAL_VAR+x} ]]; then
        echo "GLOBAL_VAR variable cannot be looked up."
        return 1
    fi       
}

##############################################################################
# write config vars with configfile template
##############################################################################
function writeConfigOptions(){
    echo "write config options"
    export SPARK_MASTER_PORT=$SPARK_MASTER_PORT
    export SPARK_MASTER_UI_PORT = $SPARK_MASTER_UI_PORT
    export JMS_IP=$JMS_IP
    export JMS_PORT=$JMS_PORT
    export APP_NAME=$APP_NAME
    export PUBLIC_ADDRESS=$PUBLIC_IP
    export METRIC_PATTERN=$METRIC_PATTERN
    export METRIC_REPORTING_INTERVAL=$METRIC_REPORTING_INTERVAL


    export SPARK_VERSION=2.3.1 
    export LOCAL_ADDRESS=$(ip route get 8.8.8.8 | awk '{print $NF; exit}')
    # Does not work for UiO dual stack network
    #export PUBLIC_ADDRESS=$(dig +short myip.opendns.com @resolver1.opendns.com)
        
    #cat /opt/docker-conf/livy.conf | envsubst > /opt/spark-2.3.1-bin-hadoop2.7/livy.conf
    #TODO use envsubst if env vars are passed to the entrypoint
    cp /opt/docker-conf/log4j.properties /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/log4j.properties
    cp /opt/docker-conf/spark-defaults.conf /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-defaults.conf
    cp /opt/docker-conf/spark-env.sh /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-env.sh
}

function init(){

    ## pre-config initialization

    # write file based config options
    writeConfigOptions

   

    ## post-config initialization

    ##TODO: check for Apache Spark if its running
}

##############################################################################



function spark_master_service(){

    

    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    
    echo "SPARK_PUBLIC_DNS=$PUBLIC_ADDRESS"  >> /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-env.sh
    #echo "SPARK_LOCAL_IP=$LOCAL_ADDRESS"  >> /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-env.sh
    echo "SPARK_LOCAL_IP=$PUBLIC_ADDRESS"  >> /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-env.sh
    echo "SPARK_MASTER_HOST=$LOCAL_ADDRESS"  >> /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-env.sh
    echo "SPARK_LOCAL_HOSTNAME=$PUBLIC_ADDRESS"  >> /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-env.sh

    # port setting
    echo "SPARK_MASTER_PORT=$SPARK_MASTER_PORT"  >> /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-env.sh

    # start spark in foregorund
    echo "SPARK_NO_DAEMONIZE" >> /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-env.sh
    
    export SPARK_CONF_DIR=/opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf

    echo "starting Apache Spark Master!"
    /opt/spark-$SPARK_VERSION-bin-hadoop2.7/sbin/start-master.sh --webui-port ${SPARK_MASTER_UI_PORT}

    # whatever blocking call 
    #tail -f /dev/null
}

function metric_agent_service(){

    #TODO: set endpoints for spark master and jms

    # Set Spark Master Metrics Endpoint    
    echo "collector.url=http://$LOCAL_ADDRESS:4040" >> /opt/metric-agent/metric.generator.properties
    
    # Set JMS Endpoint
    echo "jms.server.address=tcp://$JMS_IP" >> /opt/metric-agent/metric.generator.properties
    echo "jms.sever.port=$JMS_PORT" >> /opt/metric-agent/metric.generator.properties
    echo "collector.url.application.name=$APP_NAME" >> /opt/metric-agent/metric.generator.properties
    echo "metric.pattern=$METRIC_PATTERN" >> /opt/metric-agent/metric.generator.properties
    echo "collector.checking.time=$METRIC_REPORTING_INTERVAL" >> /opt/metric-agent/metric.generator.properties 

    java -jar /opt/metric-agent/metric-agent.jar -p /opt/metric-agent/metric.generator.properties

}

function start(){

    bgo spark_master_service metric_agent_service
    if [[ $? != 0 ]]; then
        echo "start failed. exiting now." >&2
        exit 1
    fi
}

##############################################################################
function configure(){
    echo "configure: ..."
    ## post-start configuration via service
}

##############################################################################
function main(){
    # validate env vars
    validate
    if [[ $? != 0 ]]; then 
        echo "validation failed. exiting now." >&2
        exit 1
    fi

    # initialize
    init
    if [[ $? != 0 ]]; then 
        echo "init failed. exiting now." >&2
        exit 1
    fi

    # start
    start 
    if [[ $? != 0 ]]; then
        echo "start failed. exiting now." >&2
        exit 1
    fi    

    # configure
    retry 5 5 "configure failed." configure
    if [[ $? != 0 ]]; then
        echo "cannot run configure." >&2
        exit 1
    fi

    # wait
    echo "done. now waiting for services."
    #freq=5; waitForN=-1; killTasks=0 # fail one, ignore (development mode)
    freq=5; waitForN=1; killTasks=1 #fail one, fail all (production mode)
    bgowait $freq $waitForN $killTasks
}

if [[ "$1" == "" ]]; then
    main
else
    exec "$@"
fi
